"""
MediaMeter RSS Parser v3 - –¥–ª—è Railway
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–µ—Ä—Å–æ–Ω –∏–∑ –ë–î –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏
"""

import asyncio
import hashlib
from datetime import datetime
import os
import sys

# –î–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—É—â—É—é –ø–∞–ø–∫—É –≤ path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from collectors_utils_v2 import (
        get_persons_from_db, send_to_api, extract_persons_from_text,
        analyze_sentiment, print_header, print_timestamp
    )
except ImportError:
    print("‚ùå Error: collectors_utils_v2.py not found!")
    sys.exit(1)

import feedparser

# ============ CONFIG ============

API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000")
API_KEY = os.getenv("API_KEY", "dev_key_change_in_prod")
COLLECTION_INTERVAL = int(os.getenv("COLLECTION_INTERVAL", "3600"))  # 1 —á–∞—Å

# RSS –∫–∞–Ω–∞–ª–∏
RSS_FEEDS = [
    {"name": "–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –ø—Ä–∞–≤–¥–∞", "url": "https://www.pravda.com.ua/rss/"},
    {"name": "BBC –£–∫—Ä–∞—ó–Ω–∞", "url": "https://www.bbc.com/ukrainian/index.xml"},
    {"name": "–£–∫—Ä–∏–Ω—Ñ–æ—Ä–º", "url": "https://www.ukrinform.ua/rss/all"},
]

PROCESSED_IDS = set()
PERSONS = []

# ============ FUNCTIONS ============

def get_article_id(url, title):
    """–ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π ID"""
    return hashlib.md5(f"{url}{title}".encode()).hexdigest()

async def process_feed(feed_info, persons_list):
    """–û–±—Ä–æ–±–∏—Ç–∏ RSS –∫–∞–Ω–∞–ª"""
    try:
        print(f"\nüì∞ {feed_info['name']}")
        feed = feedparser.parse(feed_info['url'])
        
        entries = feed.entries[:20]  # –í–∑—è—Ç–∏ –±—ñ–ª—å—à–µ —Å—Ç–∞—Ç–µ–π
        print(f"  Found {len(entries)} articles")
        
        processed_count = 0
        
        for entry in entries:
            try:
                title = entry.get('title', '')
                summary = entry.get('summary', '')
                link = entry.get('link', '')
                published = entry.get('published', '')
                
                if not title or not link:
                    continue
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –ø–µ—Ä—Å–æ–Ω—É
                persons = extract_persons_from_text(title + " " + summary, persons_list)
                if not persons:
                    continue
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –Ω–µ –æ–±—Ä–æ–±–∏–ª–∏
                article_id = get_article_id(link, title)
                if article_id in PROCESSED_IDS:
                    continue
                
                PROCESSED_IDS.add(article_id)
                
                # –ê–Ω–∞–ª—ñ–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ
                sentiment, score = analyze_sentiment(title + " " + summary)
                
                # Parse –¥–∞—Ç–∏
                try:
                    # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ —Ä—ñ–∑–Ω—ñ —Ñ–æ—Ä–º–∞—Ç–∏ –¥–∞—Ç
                    if published:
                        published_dt = datetime.strptime(published[:19], "%Y-%m-%dT%H:%M:%S")
                    else:
                        published_dt = datetime.now()
                except:
                    published_dt = datetime.now()
                
                # –î–∞–Ω—ñ –¥–ª—è API
                mention_data = {
                    "external_id": article_id,
                    "source_type": "news",
                    "source_id": feed_info['name'],
                    "source_title": feed_info['name'],
                    "published_at": published_dt.isoformat(),
                    "title": title[:200],
                    "content": summary[:1000],
                    "url": link,
                    "persons": persons,
                    "sentiment": {"label": sentiment, "score": score},
                }
                
                # –í—ñ–¥–ø—Ä–∞–≤–∏—Ç–∏ –Ω–∞ API
                success, status = await send_to_api(mention_data, API_BASE_URL, API_KEY)
                if success:
                    print(f"  ‚úì {title[:50]}... ({persons[0]})")
                    processed_count += 1
                else:
                    print(f"  ‚ùå Failed: {status}")
            
            except Exception as e:
                pass
        
        if processed_count > 0:
            print(f"  ‚úì Processed {processed_count} articles with tracked persons")
    
    except Exception as e:
        print(f"  ‚ùå Error: {e}")

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    global PERSONS
    
    print_header("MediaMeter RSS Parser v3 (Railway)")
    
    print(f"Configuration:")
    print(f"  API_BASE_URL: {API_BASE_URL}")
    print(f"  API_KEY: {API_KEY[:20]}...")
    print(f"  Collection interval: {COLLECTION_INTERVAL}s")
    print()
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä—Å–æ–Ω –∑ –ë–î
    PERSONS = get_persons_from_db()
    if not PERSONS:
        print("‚ùå No persons to track! Add some to database first.")
        return
    
    print(f"‚úì Tracking persons:")
    for person in PERSONS:
        print(f"  ‚Ä¢ {person}")
    print()
    
    iteration = 0
    while True:
        iteration += 1
        print(f"\n{'='*60}")
        print(f"‚è± Iteration #{iteration} - {print_timestamp()}")
        print(f"{'='*60}")
        
        try:
            for feed_info in RSS_FEEDS:
                await process_feed(feed_info, PERSONS)
        except Exception as e:
            print(f"\n‚ùå Iteration error: {e}")
        
        print(f"\n‚è≥ Waiting {COLLECTION_INTERVAL}s until next collection...")
        await asyncio.sleep(COLLECTION_INTERVAL)

if __name__ == "__main__":
    try:
        print("‚úì Starting RSS Parser v3...")
        print()
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\n‚úì Parser stopped by user")
    except Exception as e:
        print(f"\n\n‚ùå Fatal error: {e}")
        sys.exit(1)
